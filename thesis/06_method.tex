
\section{Method}\label{Sec:Method}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   Forecasting methods   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Forecasting methods}\label{Sec:Method;Subsec:Forecast}

Based on an extensive literature review three different prediction techniques are proposed to forecast energy consumption and production ahead based only on historical (and potentially calendar) features of the Smart Meter time series: Pooling-based deep recurring neural net-work (PDRNN) based on the procedure outlined by \citet{Shi:2017}, sparse autoregressive LASSO as developed and implemented by \citet{Li:2017}, and Kernel-Wavelet-Functional method (KWF) following the idea of \citet{Auder:2018} of using discrete wavelet transform and clustering before applying the KWF method.
As the data used to train the models is in 3-minute intervals, but the prediction will have to be made 15 minutes ahead, all forecasting models will be used to make 5 sequential one-step ahead predictions. These 5 predictions will then be aggregated into a single forecast for the whole 15-minute interval and compared to the actual consumption/production within this 15-minute interval (i.e., the aggregation of the 5 corresponding 3-minute consumption values).


%%%%%%%%%%%
\subsubsection{Benchmark models}

What about flat forecast?


The predictions generated with the above-mentioned forecasting techniques will be benchmarked against the prediction of a simple persistence model, i.e., the naïve predictor

\begin{equation} \label{Eq:naivepred}
\widehat{x}_{t+1}=x_t
\end{equation}

which is a good predictor for time horizons of minutes in the context of energy forecasting \citep{Pinson:2012}. Furthermore, as second benchmark an ARIMA model of first-order difference is used which is defined in polynomial form as

\begin{equation} \label{Eq:ARIMA}
x_t-x_{t-1}=(1-b)x_t
\end{equation}

\citep{Box:1990}.


%%%%%%%%%%%
\subsubsection{Long short-term memory recurring neural network}



%%%%%%%%%%%
\subsubsection{Sparse auto-regressive LASSO}



%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   Error measures   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Error measures}\label{Sec:Method;Subsec:Error}

Error measures play a essential role in any prediction task. Also called performance metrics, these measures are used to quantify the accuracy of the prediction generated by a forecasting model \citep{zor:2017}. Without assessing the prediction accuracy through error measures, it is impossible to quantify whether the proposed forecasting technique is an improvement compared to the benchmark models \citep{Meer:2018}. Moreover, error measures are used by supervised machine learning algorithms to assess the prediction accuracy in cross-validation and accordingly adjust their parameters. However, there is a wide variaty of error measures available and actively used in the research of energy forecasting. \citet{zor:2017} reviewed energy forecasting literature published in 2017 and found eight different error measures used to assess the forecasting accuracy. Among those, mean absolute percentage error was used in 83 \% of the studies, with mean absolute error and root mean squared error coming third and second with 32 and 31 \% respectively. As these results suggest, there is, unfortunately, a lack of standardization in the field of energy forecasting regarding the usage of the various available error measures \citep{Meer:2018}. This is aggravated by the fact, different error measures are appropriate in different use cases and cannot be generally applied without careful consideration. Therefore, the following section will introduce the error measures used in the research at hand and discuss their advantages and disadvantages. Following the suggestion of \citet{Hoff:2013} several performance metrics will be used to evaluate the quality of the forecasts. The choice of performance metrics is mostly guided by the compilation provided by \citet{Meer:2018}.

Error measures can be classified into capturing absolute or percentage errors \citep{Hoff:2013}. Absolute error measures are, for example, mean absolute error (MAE) and root mean squared error (RMSE) which are quite popular as performance metrics for energy forecasts \citet{zor:2017}. One disadvantage of these measures is that they are not scale independent which makes them unsuitable to compare the prediction accuracy on different time series. However, they can be suitable for cross-validation and the comparison of sophisticated forecasting techniques with benchmark models on the same time series. Moreover, they do not rely on denominator-related assumptions as percentage error measures do which makes them more robust \citep{Hoff:2013}. Absolute error measures can be formulated in terms of a vector function 

\begin{equation} \label{Eq:vectorfunction}
    E=F\left(\vec{f}, \vec{x}\right),
\end{equation}

\noindent where $\vec{f}$ and $\vec{x}$ are the forecasted and actual data vectors respectively \citep{Haben:2014}. The metric $F$ is then the absolute p-norm,

\begin{equation} \label{Eq:pnorm}
    E_p=\left\lVert\vec{f}-\vec{x}\right\rVert_p=\biggl(\sum_{i=1}^N \left|f_i-x_i\right|^p\biggr)^{1/p},
\end{equation}

\noindent for $p≥1$ \citep[][p. 52]{golub:2012}. The MAE belongs to this type of error and is defined as the average of the absolute differences between the predicted and true values \citet{Hoff:2013}:

\begin{equation} \label{Eq:MAE}
\text{MAE}=\frac{1}{N}\sum_{t=1}^N\left|\widehat{x}_t-x_t\right|,    
\end{equation}

\noindent where N is the length of the forecasted time series, $\widehat{x}_t$ the forecasted value and $x_t$ the observed value. This is equivalent to equation \ref{Eq:pnorm} with $p=1$. Similar to the MAE and also of the p-norm type of error measure is the RMSE. Instead of summing up the \textit{absolute} differences, the RMSE is defined as the square root of the average \textit{squared} differences (which is equivalent to $p=2$ in equation \ref{Eq:pnorm}):

\begin{equation} \label{Eq:RMSE}
\text{RMSE}=\sqrt{\frac{1}{N}\sum_{t=1}^N\left(\widehat{x}_t-x_t\right)^2}.
\end{equation}

The RMSE, thus, puts more weight on large deviations between forecast and observation than the MAE \citep{Meer:2018}. Therefore, the RMSE is more suitable in the presence of a lot of noise, as it does not mask a small amount of large errors in the presence of a majority of small errors as the MAE does \citep{Zhang:2015}.

Even though the MAE and RMSE are widely used, they are not useful to compare the forecast accuracy across different time series as they are not scale independent \citep{Meer:2018}. Therefore, it is reasonable to complement them by percentage error measures which are normalized by a denominator to make them scale independent. However, depending on the application, there may be several denominators that could be used, each coming with certain advantages and disadvantages. \citet{Hoff:2013}, for example, found that the choice of the denominator influences the calculated error results of solar irradiance forecasts substantially. Generally, the denominator fall into two categories: (1) It can be a single number that is representative of the forecasted time series (e.g., the maximum value of the forecasted time series, the average value of the forecasted time series or the capacity as the maximum constraint of the electrical system under consideration) as proposed by \citet{Hoff:2013} and agreed on by \citet{Meer:2018}. (2) The denominator can be different for every pair of true and predicted value (i.e., the true value is used as denominator for each pair of true and predicted value) as done by \citet{xie:2018} and agreed on by \citet{zor:2017}. 

the mean absolute percentage error (MAPE) and the normalized root mean squared error (NRMSE). Investigating forecasting error measures for PV power plants, \citet{Hoff:2013} conclude that normalizing the MAE by the average output of a PV power plant is most desirable to compute the MAPE. Even though \citet{Meer:2018} did not find any literature supporting this for consumption forecasting, they suggest that the same reasoning should hold here as well. Therefore, the MAPE and NRMSE are formulated as follows

%%%%%%%%%%%
\subsubsection{MAE and RMSE}



%%%%%%%%%%%
\subsubsection{MAPE and NRMSE}

Even though the MAE and RMSE are widely used, they are not useful to compare the forecast accuracy across studies as these measures are not scale independent. Therefore, they will be complemented by the mean absolute percentage error (MAPE) and the normalized root mean squared error (NRMSE). Investigating forecasting error measures for PV power plants, \citet{Hoff:2013} conclude that normalizing the MAE by the average output of a PV power plant is most desirable to compute the MAPE. Even though \citet{Meer:2018} did not find any literature supporting this for consumption forecasting, they suggest that the same reasoning should hold here as well. Therefore, the MAPE and NRMSE are formulated as follows

\begin{equation} \label{Eq:MAPE}
\text{MAPE}=\frac{100}{N}\sum_{i=1}^N\left|\frac{\widehat{x}_i-x_i}{\bar{x}}\right|,
\end{equation}

\begin{equation} \label{Eq:NRMSE}
\text{NRMSE}=\sqrt{\frac{100}{N}\sum_{i=1}^N\left(\frac{\widehat{x}_i-x_i}{\bar{x}}\right)^2}.
\end{equation}

where $\bar{x}$ is the average of the observed time series. For MAPE this is equivalent to dividing the sum of absolute errors by the sum of all values contained in the time series. This formulation avoids the problem of the classical MAPE formulation using $x_t$ to normalize, that the fraction $\frac{\widehat{x}_i-x_i}{\bar{x}_t}$ is not defined if $x_t=0$ and is for example used in \citet{Yamin:2004}.



%%%%%%%%%%%
\subsubsection{MASE}

Finally, the mean absolute scaled error (MASE) by \citet{Hyndman:2006} will be used to assess the forecasts as it has the advantage of being applicable even if the time series includes a great number of meaningless zero values (e.g., night-time PV energy production). Additionally, the MASE overcomes the issue that MAPE puts a heavier penalty on negative errors. It normalizes the MAE with the forecast made by the persistence model and is defined as

\begin{equation} \label{Eq:MASE}
\text{MASE}=\frac{\text{MAE}}{\frac{1}{n-1}\sum_{i=2}^N\left|x_i-x_{i-1}\right|}.
\end{equation}



Additionally, skewness and kurtosis of forecasting errors’ distribution will be calculated to further assess the quality of the predictions made by different forecasting techniques.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%   Market simulation   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Market simulation}\label{Sec:Method;Subsec:Market}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

    \item How was the data analyzed ?

    \item Present the underlying economic model/theory and
        give reasons why it is suitable to answer the given problem.

    \item Present econometric/statistical estimation method and
        give reasons why it is suitable to answer the given problem.

    \item Allows the reader to judge the validity of the study and its findings.

    \item Depending on the topic this section can also be split up into separate sections.

\end{itemize}
